{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuliaRenovato/Projeto123/blob/main/PRO_C126_Project_Boilerplate_Code(pt).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O **Tiro com Arco** é um jogo em que os jogadores atiram flechas de ponta afiada em um alvo redondo com 10 anéis.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\">"
      ],
      "metadata": {
        "id": "nUWO5QkC_g-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema de Aprendizado por Reforço (RL) a Resolver\n",
        "Acerte o centro do alvo que proporciona a recompensa máxima"
      ],
      "metadata": {
        "id": "5QtHLAqv3wP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Número de **Estados**: ?\n",
        "\n",
        "Número de **Ações**: ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Osb6FQ74YZtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importe as bibliotecas\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de Recompensas\n",
        "A Matriz de Recompensas representa os estados como linhas e as ações como colunas com os respectivos valores de recompensas atribuídos a um determinado par de estado e ação."
      ],
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie a matriz de recompensas\n",
        "rewards = np.array([10,9,8,7,6,5,4,3,2,1])\n",
        "print(rewards)"
      ],
      "metadata": {
        "id": "OUqPgOl0eh2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85c3b17-1bd9-45d5-aeca-4c3ad8e6f456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10  9  8  7  6  5  4  3  2  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Ações Aleatoriamente"
      ],
      "metadata": {
        "id": "Af-CAmdfkDQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defina shoot()\n",
        "def shoot():\n",
        "  return np.random.randint(0,10)"
      ],
      "metadata": {
        "id": "ibSLCyMyigmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = shoot()\n",
        "print(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5v9dMJ8Oar7",
        "outputId": "9360062f-167f-4385-97bf-5b759416098f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matriz Q\n",
        "O **Aprendizado Q** é um algoritmo de aprendizado por reforço. Dado o estado atual, ele ajuda a encontrar a melhor ação a ser tomada pelo agente.\n",
        "\n",
        "A **Matriz Q** representa a recompensa recebida após uma ação específica no estado atual. Inicialmente, todos os elementos da matriz Q estão zerados.\n"
      ],
      "metadata": {
        "id": "JXKyVT28hHoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crie a matriz Q\n",
        "q_matrix = np.zeros([10])\n",
        "print(q_matrix)"
      ],
      "metadata": {
        "id": "aNYwOV7ogtw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3dbe71-0ac7-4788-cf83-51626049c368"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Execute uma Ação"
      ],
      "metadata": {
        "id": "0c95A4SOkGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defina take_action\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "\n",
        "     #Chame a função shoot() para obter a ação\n",
        "     action = shoot()\n",
        "\n",
        "     #Imprima a ação\n",
        "     print(action)\n",
        "\n",
        "     #Obtenha a recompensa correspondente usando a matriz de recompensas\n",
        "     reward = reward_matrix[action]\n",
        "\n",
        "     #Imprima a recompensa\n",
        "     print(reward)\n",
        "\n",
        "     return action, reward"
      ],
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action, reward = take_action(rewards)\n",
        "print(reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TKTJfS8Rsgl",
        "outputId": "d890b176-13f5-4d8e-cd76-91065aa67d57"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "9\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atualize a Matriz Q"
      ],
      "metadata": {
        "id": "cKy1VkgO4ZhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defina o método run_episode()\n",
        "\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  #use o loop for para percorrer o número de tentativas\n",
        "\n",
        "        #imprima o número do tiro\n",
        "\n",
        "        #chame o método take_action para obter a ação\n",
        "\n",
        "        #aumente a pontuação\n",
        "\n",
        "        #imprima o número do tiro final\n",
        "\n",
        "  #Atualize a matriz Q\n",
        "\n",
        "  #retorne a matriz Q atualizada\n"
      ],
      "metadata": {
        "id": "_U6NFICkhGMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chame o método run_episode para verificar a matriz Q final de um episódio\n",
        "\n"
      ],
      "metadata": {
        "id": "n5pcUO-5pJ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinar\n",
        "\n",
        "Crie uma função que execute um número de jogos, execute cada jogo por um determinado número de vezes e calcule as recompensas médias para cada vez."
      ],
      "metadata": {
        "id": "0dVY734TlZBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defina a função train()\n",
        "\n",
        "def train(episodes):\n",
        "  #Use o loop for para percorrer os episódios\n",
        "\n",
        "    #Inicialize a variável total_reward\n",
        "\n",
        "    #Imprima \"Episódio Inicia\" com o número do episódio\n",
        "\n",
        "    #Chame o método run_episode() para obter a matriz q de um episódio\n",
        "\n",
        "    #A recompensa do episódio será a soma de todas as recompensas de um episódio\n",
        "\n",
        "    #imprima a recompensa do episódio\n",
        "\n",
        "    #A recompensa total será a soma de todas as recompensas do episódio\n",
        "\n",
        "\n",
        "  #retorne total_reward\n"
      ],
      "metadata": {
        "id": "smfOXGYZlp7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Treine 1000 episódios"
      ],
      "metadata": {
        "id": "7WYH_ioykcBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute a função train() para 1000 episódios\n",
        "\n",
        "\n",
        "#Imprima a recompensa média\n"
      ],
      "metadata": {
        "id": "cbjnp3PSovsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão:\n",
        "\n",
        "Isso nos dá uma boa ideia sobre o desempenho geral do aprendizado de reforço mais simples com o problema **um estado - múltiplas ações**, também conhecido como problema \"**K-Armed Bandit**\".\n",
        "\n",
        "Um dos principais casos de uso desse tipo de problema pode ser visto na seleção do anúncio certo entre muitos a serem exibidos na página web. A máquina pode ser ensinada a escolher o melhor anúncio com mais cliques do usuário!!\n",
        "\n"
      ],
      "metadata": {
        "id": "-jBnwogyuJP0"
      }
    }
  ]
}